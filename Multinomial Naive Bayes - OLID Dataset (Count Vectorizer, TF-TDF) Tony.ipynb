{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>processed_tweets</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>targeted_or_not</th>\n",
       "      <th>target_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>['@user', 'ask', 'native', 'american', 'take']</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>['@user', '@user', 'go', 'home', 'drunk', '@us...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>['amazon', 'investigate', 'chinese', 'employee...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>['@user', 'someone', 'piece', 'shit', 'volcano']</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>['@user', '@user', 'obama', 'want', 'liberal',...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                   processed_tweets  sentiment  \\\n",
       "0  86426     ['@user', 'ask', 'native', 'american', 'take']          1   \n",
       "1  90194  ['@user', '@user', 'go', 'home', 'drunk', '@us...          1   \n",
       "2  16820  ['amazon', 'investigate', 'chinese', 'employee...          0   \n",
       "3  62688   ['@user', 'someone', 'piece', 'shit', 'volcano']          1   \n",
       "4  43605  ['@user', '@user', 'obama', 'want', 'liberal',...          0   \n",
       "\n",
       "   targeted_or_not  target_type  \n",
       "0              0.0          NaN  \n",
       "1              1.0          1.0  \n",
       "2              NaN          NaN  \n",
       "3              0.0          NaN  \n",
       "4              NaN          NaN  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = ['id', 'processed_tweets', 'sentiment', 'targeted_or_not', 'target_type']\n",
    "df_train = pd.read_csv('processed_trainingset.csv')[column_names]\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>processed_tweets</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>targeted_or_not</th>\n",
       "      <th>target_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15923</td>\n",
       "      <td>['whoisq', 'wherestheserver', 'dumpnike', 'dec...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27014</td>\n",
       "      <td>['constitutionday', 'revere', 'conservative', ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30530</td>\n",
       "      <td>['foxnews', 'nra', 'maga', 'potus', 'trump', '...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13876</td>\n",
       "      <td>['watch', 'boomer', 'get', 'news', 'still', 'p...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60133</td>\n",
       "      <td>['nopasaran', 'unity', 'demo', 'oppose', 'farr...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                   processed_tweets  sentiment  \\\n",
       "0  15923  ['whoisq', 'wherestheserver', 'dumpnike', 'dec...          1   \n",
       "1  27014  ['constitutionday', 'revere', 'conservative', ...          0   \n",
       "2  30530  ['foxnews', 'nra', 'maga', 'potus', 'trump', '...          0   \n",
       "3  13876  ['watch', 'boomer', 'get', 'news', 'still', 'p...          0   \n",
       "4  60133  ['nopasaran', 'unity', 'demo', 'oppose', 'farr...          1   \n",
       "\n",
       "   targeted_or_not  target_type  \n",
       "0              1.0          3.0  \n",
       "1              NaN          NaN  \n",
       "2              NaN          NaN  \n",
       "3              NaN          NaN  \n",
       "4              1.0          2.0  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('processed_testset.csv')[column_names]\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now convert the datasets into a form that can be used by the ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "temp = list(df_train['processed_tweets'])\n",
    "list_of_train_tweets = [None]*df_train.shape[0]\n",
    "for i in range(df_train.shape[0]):\n",
    "    list_of_train_tweets[i] = \" \".join(literal_eval(temp[i]))\n",
    "    \n",
    "temp = list(df_test['processed_tweets'])\n",
    "list_of_test_tweets = [None]*df_test.shape[0]\n",
    "for i in range(df_test.shape[0]):\n",
    "    list_of_test_tweets[i] = \" \".join(literal_eval(temp[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to create a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(corpus):\n",
    "    '''\n",
    "    Creates a vocabulary out of the corpus.\n",
    "    Input: List of strings, where each string is a processed tweet\n",
    "    Output: List of words (vocabulary)\n",
    "    '''\n",
    "    # Initialize the vocabulary as an empty set\n",
    "    vocab = {}\n",
    "    for doc in corpus:\n",
    "        # For every document, take the set union of the words in that document\n",
    "        # with words already in the vocab\n",
    "        words = doc.split(\" \")\n",
    "        vocab = set(vocab).union(words)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels for Task A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_training_labels = list(df_train['sentiment'])\n",
    "sk_test_labels = list(df_test['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the tweets into vectors based on frequency of occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a vocabulary\n",
    "vocab = create_vocabulary(list_of_train_tweets)\n",
    "count_vector = CountVectorizer(vocabulary=vocab)\n",
    "\n",
    "\n",
    "# Obtains a numpy array of frequencies of words in the document.\n",
    "sk_training_tweets = count_vector.fit_transform(list_of_train_tweets).toarray()\n",
    "sk_test_tweets = count_vector.fit_transform(list_of_test_tweets).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes approach with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Leaving the parameters at default ensures that the model learns the prior probabilities from the\n",
    "# data and that it uses Laplace smoothing to deal with new words.\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(sk_training_tweets, sk_training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training set : 88.13392043729122 %\n",
      "Accuracy on the test set : 79.18604651162791 %\n"
     ]
    }
   ],
   "source": [
    "# Performance on the training set\n",
    "print(\"Accuracy on the training set : {} %\".format(mnb.score(sk_training_tweets, sk_training_labels)*100))\n",
    "\n",
    "# Performance on the training set\n",
    "print(\"Accuracy on the test set : {} %\".format(mnb.score(sk_test_tweets, sk_test_labels)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinimial Naive Bayes approach with TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training set : 77.7027027027027 %\n",
      "Accuracy on the test set : 75.81395348837209 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Create a vocabulary\n",
    "vocab = create_vocabulary(list_of_train_tweets)\n",
    "tfidf_vector = TfidfVectorizer(vocabulary=vocab)\n",
    "\n",
    "\n",
    "# Obtains a numpy array of frequencies of words in the document.\n",
    "sk_training_tweets2 = tfidf_vector.fit_transform(list_of_train_tweets).toarray()\n",
    "sk_test_tweets2 = tfidf_vector.fit_transform(list_of_test_tweets).toarray()\n",
    "\n",
    "mnb2 = MultinomialNB()\n",
    "\n",
    "mnb2.fit(sk_training_tweets2, sk_training_labels)\n",
    "\n",
    "# Performance on the training set\n",
    "print(\"Accuracy on the training set : {} %\".format(mnb2.score(sk_training_tweets2, sk_training_labels)*100))\n",
    "\n",
    "# Performance on the training set\n",
    "print(\"Accuracy on the test set : {} %\".format(mnb2.score(sk_test_tweets2, sk_test_labels)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained better a higher accuracy for subtask A by using Multinomial Naive "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env37",
   "language": "python",
   "name": "env37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

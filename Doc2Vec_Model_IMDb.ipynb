{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Doc2Vec Model IMDb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "18AapjBsqWjPgh-4Bp0hrVIMo-mFSSHGo",
      "authorship_tag": "ABX9TyPe30GRTxUiwdZi6sgSZUX0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tfredrick112/sentiment-classification-ml-course-project/blob/master/Doc2Vec_Model_IMDb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyLRTCaSQsIV",
        "colab_type": "text"
      },
      "source": [
        "###Preprocessing Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQPTWpAKjn2v",
        "colab_type": "text"
      },
      "source": [
        "####Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9kEe7toXrGg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "73f07da7-a691-418d-d970-fcb4395d0823"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import re\n",
        "\n",
        "import unicodedata\n",
        "import string\n",
        "from time import time\n",
        "\n",
        "import io\n",
        "import tarfile\n",
        "import os.path\n",
        "import smart_open\n",
        "import gensim.utils"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrZA3MvPVVe7",
        "colab_type": "text"
      },
      "source": [
        "####Converting to Lowercase and Removing Punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYFVbHqzUmBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
        "\n",
        "def my_word_tokenize(doc):\n",
        "    new_doc = \"\"\n",
        "    characters_to_remove = string.punctuation.replace(\"'\", \"\") + \"\\n\"\n",
        "    new_doc = ''.join([ch if ch not in characters_to_remove else \" \" for ch in doc])\n",
        "    return new_doc.split()\n",
        "\n",
        "def basic_preprocess_1(doc):\n",
        "    '''\n",
        "    Input: doc (A comment, which is string)\n",
        "    Output: List of words in the preprocessed document\n",
        "    Basic preprocessing includes:\n",
        "    a) converting to lowercase\n",
        "    b) removing newline characters\n",
        "    c) removing all punctuation\n",
        "    d) normalizing to NFKC form\n",
        "    '''\n",
        "    \n",
        "    # Normalization and conversiom to lowercase\n",
        "    doc = unicodedata.normalize('NFKC', doc).lower()\n",
        "\n",
        "    # Remove HTML tags\n",
        "    doc = re.sub('<[^>]*>', ' ', doc)\n",
        "    \n",
        "    # Maps all punctuation marks to space\n",
        "    temp = string.punctuation.replace('@', '').replace(\"'\", \"\")\n",
        "    table = str.maketrans(temp, ' '*len(temp))\n",
        "    doc = doc.translate(table)\n",
        "    \n",
        "    # A list of all the words in the document after removing punctuation.\n",
        "    tokens = TweetTokenizer().tokenize(doc)\n",
        "\n",
        "    stripped = [w for w in tokens if (w!=' ' and w!='' and w.isalnum()) or w=='@user']\n",
        "    return stripped"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S84zgYV8Vmpa",
        "colab_type": "text"
      },
      "source": [
        "####Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRityWdRVL2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import  wordnet\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    '''\n",
        "    This function is used to convert the Part of Speech tags returned by nltk.pos_tag\n",
        "    function to the wordnet POS tags.\n",
        "    '''\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "        # Instead of returning None, I am returning the noun tag, because, by default\n",
        "        # the WordNetLemmatizer assumes that the POS tag is noun.\n",
        "        \n",
        "def lemmatize(doc_words):\n",
        "    '''\n",
        "    Input: doc_words(A comment) It is a list of words\n",
        "    Output: A list of words in the document,after lemmatization\n",
        "    '''\n",
        "    # Creating Part-of-Speech tags for the words\n",
        "    pos_tags = nltk.pos_tag(doc_words)\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lem_words = [lemmatizer.lemmatize(word, pos = get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
        "    return lem_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvs8qy4dX7Zh",
        "colab_type": "text"
      },
      "source": [
        "####Remove Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yed1jo7aX9Rd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removal of stopwords\n",
        "from nltk.corpus import stopwords\n",
        "def remove_stopwords(doc_words):\n",
        "    '''\n",
        "    Input: doc_words(a comment) It is a list of words\n",
        "    Output: List of words in that document, after removing stop words.\n",
        "    '''\n",
        "    words_to_remove = set(stopwords.words('english'))\n",
        "    words = [word for word in doc_words if word not in words_to_remove]\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wiy5xhjW6iE",
        "colab_type": "text"
      },
      "source": [
        "####Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b42ej6UfW6HL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_1(doc):\n",
        "    '''\n",
        "    Input: doc (A comment)\n",
        "    Output: List of words (after preprocessing)\n",
        "    '''\n",
        "    preprocessed_doc = basic_preprocess_1(doc.replace(\"â€™\", \"'\"))\n",
        "    lemmatized_doc = lemmatize(preprocessed_doc)\n",
        "    final_preprocessed_doc = remove_stopwords(lemmatized_doc)\n",
        "    return final_preprocessed_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yNp-gvd3oyo",
        "colab_type": "text"
      },
      "source": [
        "###Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk3b9HsV4Qbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "SentimentDocument = collections.namedtuple('SentimentDocument', 'words tags split sentiment')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjY_sjHW3nei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_dataset(url='http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'):\n",
        "    fname = url.split('/')[-1]\n",
        "\n",
        "    if os.path.isfile(fname):\n",
        "       return fname\n",
        "\n",
        "    with smart_open.open(url, \"rb\", ignore_ext=True) as fin:\n",
        "        with smart_open.open(fname, 'wb', ignore_ext=True) as fout:\n",
        "            while True:\n",
        "                buf = fin.read(io.DEFAULT_BUFFER_SIZE)\n",
        "                if not buf:\n",
        "                    break\n",
        "                fout.write(buf)\n",
        "\n",
        "    return fname\n",
        "\n",
        "def create_sentiment_document(name, text, index):\n",
        "    _, split, sentiment_str, _ = name.split('/')\n",
        "    sentiment = {'pos': 1.0, 'neg': 0.0, 'unsup': None}[sentiment_str]\n",
        "\n",
        "    if sentiment is None:\n",
        "        split = 'extra'\n",
        "\n",
        "    tokens = gensim.utils.to_unicode(text).split()\n",
        "    return SentimentDocument(tokens, [index], split, sentiment)\n",
        "\n",
        "def extract_documents():\n",
        "    fname = download_dataset()\n",
        "\n",
        "    index = 0\n",
        "\n",
        "    with tarfile.open(fname, mode='r:gz') as tar:\n",
        "        for member in tar.getmembers():\n",
        "            if re.match(r'aclImdb/(train|test)/(pos|neg|unsup)/\\d+_\\d+.txt$', member.name):\n",
        "                member_bytes = tar.extractfile(member).read()\n",
        "                member_text = member_bytes.decode('utf-8', errors='replace')\n",
        "                assert member_text.count('\\n') == 0\n",
        "                yield create_sentiment_document(member.name, member_text, index)\n",
        "                index += 1\n",
        "\n",
        "alldocs = list(extract_documents())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue_OFFJ-4Jb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Select only the review text (preprocessed) and the sentiment label\n",
        "train_docs = [{'doc': doc.words, 'sentiment_numeric':int(doc.sentiment)} for doc in alldocs if doc.split == 'train']\n",
        "test_docs = [{'doc': doc.words, 'sentiment_numeric': int(doc.sentiment)} for doc in alldocs if doc.split == 'test']\n",
        "\n",
        "# Dataframe consisting of training examples\n",
        "df_train = pd.DataFrame(train_docs)\n",
        "\n",
        "# Dataframe consisting of test examples\n",
        "df_test = pd.DataFrame(test_docs)\n",
        "\n",
        "# All the docs in the dataset will be used to train the Doc2Vec model\n",
        "doc2vec_model_docs = [doc.words for doc in alldocs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0bJpr4y5qXt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82d6b580-fa2d-41e0-be53-6d9c13ddf5b1"
      },
      "source": [
        "import gensim\n",
        "\n",
        "def load_reviews_train(processed_reviews):\n",
        "    for i, review in enumerate(processed_reviews):\n",
        "        yield gensim.models.doc2vec.TaggedDocument(review, str(i))\n",
        "\n",
        "train_corpus = list(load_reviews_train(doc2vec_model_docs))\n",
        "print(\"Number of training sequences (for Doc2Vec) = \",len(train_corpus))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sequences (for Doc2Vec) =  100000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2ClfVE2TiQt",
        "colab_type": "text"
      },
      "source": [
        "###Model Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWmT9wyRTUrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import multiprocessing\n",
        "import gensim.models.doc2vec\n",
        "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
        "from gensim.models.doc2vec import Doc2Vec\n",
        "\n",
        "vector_dimensions = 100\n",
        "common_kwargs = dict(\n",
        "    vector_size=vector_dimensions, epochs=25, min_count=2,\n",
        "    sample=0, workers=multiprocessing.cpu_count(), negative=5, hs=0,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih1-0_wgUmwa",
        "colab_type": "text"
      },
      "source": [
        "###Train Doc2Vec model (PV-DM model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eUmUcaCUjQu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2f3d6a94-4b01-4039-d7a7-867178668ef6"
      },
      "source": [
        "# PV-DM with default averaging\n",
        "start = time()\n",
        "model = Doc2Vec(dm=1, window=10, alpha=0.05, comment='alpha=0.05', **common_kwargs)\n",
        "model.build_vocab(train_corpus)\n",
        "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "end = time()\n",
        "print(\"Time taken to train the Doc2Vec model = {} minutes\".format((end - start)/60))\n",
        "\n",
        "# Save the model\n",
        "from gensim.test.utils import get_tmpfile\n",
        "fname = get_tmpfile(\"/content/drive/My Drive/SentimentClassification/doc2vec_pvdm_large\")\n",
        "model.save(fname)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken to train the Doc2Vec model = 32.177907117207845 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBnlvcMMUv1F",
        "colab_type": "text"
      },
      "source": [
        "###Infer Document Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceDJSMNLTlv-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6a0cf74b-db17-4df1-ce3f-7a51dd10781c"
      },
      "source": [
        "# Infer vectors\n",
        "from gensim.models import Doc2Vec\n",
        "\n",
        "# Load the saved model\n",
        "model = Doc2Vec.load(\"/content/drive/My Drive/SentimentClassification/doc2vec_pvdm_large\")\n",
        "\n",
        "# Number of training examples (for binary classification)\n",
        "M = df_train.shape[0]\n",
        "print(\"Number of training examples for binary classification =\", M)\n",
        "# Number of test examples\n",
        "Mtest = df_test.shape[0]\n",
        "print(\"Number of test examples for binary classification =\", Mtest)\n",
        "\n",
        "X_train = np.zeros((M, vector_dimensions))\n",
        "X_test = np.zeros((Mtest, vector_dimensions))\n",
        "y_train = np.array(df_train['sentiment_numeric'])\n",
        "y_test = np.array(df_test['sentiment_numeric'])\n",
        "\n",
        "# Infer vectors for training examples\n",
        "training_reviews = list(df_train['doc'])\n",
        "for i, review in enumerate(training_reviews):\n",
        "    X_train[i] = model.infer_vector(review)\n",
        "\n",
        "# Infer vectors for test examples\n",
        "test_reviews = list(df_test['doc'])\n",
        "for i, review in enumerate(test_reviews):\n",
        "    X_test[i] = model.infer_vector(review)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples for binary classification = 25000\n",
            "Number of test examples for binary classification = 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOxMicZqWALH",
        "colab_type": "text"
      },
      "source": [
        "###Binary Classification (Logistic Regression)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hnZYb4AV-wa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "aab5dd51-1268-41b6-993b-c1421b871347"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "clf = LogisticRegression().fit(X_train, y_train)\n",
        "y_pred_train = clf.predict(X_train)\n",
        "y_pred_test = clf.predict(X_test)\n",
        "\n",
        "# Training accuracy\n",
        "print(\"Training accuracy =\", round(clf.score(X_train, y_train), 4) * 100, \"%\")\n",
        "\n",
        "# Test accuracy\n",
        "print(\"Test accuracy =\", round(clf.score(X_test, y_test), 4) * 100, \"%\\n\")\n",
        "\n",
        "# F1 score\n",
        "print(\"Training F1 score =\", round(f1_score(y_train, y_pred_train), 3))\n",
        "print(\"Test F1 score =\", round(f1_score(y_test, y_pred_test), 3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy = 87.41 %\n",
            "Test accuracy = 87.13 %\n",
            "\n",
            "Training F1 score = 0.874\n",
            "Test F1 score = 0.874\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etlKPI_Zj4pU",
        "colab_type": "text"
      },
      "source": [
        "###Train Doc2Vec model (PV-DBOW)\n",
        "\n",
        "Distributed Bag of Words model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEa8W-yVafsv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aacfd009-3986-42e4-91da-b2c78aaaab79"
      },
      "source": [
        "# PV-DBOW\n",
        "start = time()\n",
        "model2 = Doc2Vec(dm=0, **common_kwargs)\n",
        "model2.build_vocab(train_corpus)\n",
        "model2.train(train_corpus, total_examples=model2.corpus_count, epochs=model2.epochs)\n",
        "end = time()\n",
        "print(\"Time taken to train the Doc2Vec model = {} minutes\".format((end - start)/60))\n",
        "\n",
        "# Save the model\n",
        "from gensim.test.utils import get_tmpfile\n",
        "fname = get_tmpfile(\"/content/drive/My Drive/SentimentClassification/doc2vec_pvdbow_large\")\n",
        "model2.save(fname)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken to train the Doc2Vec model = 73.1448712269465 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBUAfsS1k4lB",
        "colab_type": "text"
      },
      "source": [
        "###Infer Document Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BW-PoYzlk8AL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "412e26ec-2b08-4fcd-e718-8179c77f11fc"
      },
      "source": [
        "# Infer vectors\n",
        "from gensim.models import Doc2Vec\n",
        "\n",
        "# Load the saved model\n",
        "model2 = Doc2Vec.load(\"/content/drive/My Drive/SentimentClassification/doc2vec_pvdbow_large\")\n",
        "\n",
        "# Number of training examples (for binary classification)\n",
        "M = df_train.shape[0]\n",
        "print(\"Number of training examples for binary classification =\", M)\n",
        "# Number of test examples\n",
        "Mtest = df_test.shape[0]\n",
        "print(\"Number of test examples for binary classification =\", Mtest)\n",
        "\n",
        "X_train2 = np.zeros((M, vector_dimensions))\n",
        "X_test2 = np.zeros((Mtest, vector_dimensions))\n",
        "y_train2 = np.array(df_train['sentiment_numeric'])\n",
        "y_test2 = np.array(df_test['sentiment_numeric'])\n",
        "\n",
        "# Infer vectors for training examples\n",
        "training_reviews = list(df_train['doc'])\n",
        "for i, review in enumerate(training_reviews):\n",
        "    X_train2[i] = model2.infer_vector(review)\n",
        "\n",
        "# Infer vectors for test examples\n",
        "test_reviews = list(df_test['doc'])\n",
        "for i, review in enumerate(test_reviews):\n",
        "    X_test2[i] = model2.infer_vector(review)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples for binary classification = 25000\n",
            "Number of test examples for binary classification = 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0i9rbfRlcFs",
        "colab_type": "text"
      },
      "source": [
        "###Binary Classification (Logistic Regression)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fbkao47clY6q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "cbdf17e1-4c83-4091-d09a-4ecf45579f45"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "clf = LogisticRegression().fit(X_train2, y_train2)\n",
        "y_pred_train2 = clf.predict(X_train2)\n",
        "y_pred_test2 = clf.predict(X_test2)\n",
        "\n",
        "# Training accuracy\n",
        "print(\"Training accuracy =\", round(clf.score(X_train2, y_train2), 4) * 100, \"%\")\n",
        "\n",
        "# Test accuracy\n",
        "print(\"Test accuracy =\", round(clf.score(X_test2, y_test2), 4) * 100, \"%\\n\")\n",
        "\n",
        "# F1 score\n",
        "print(\"Training F1 score =\", round(f1_score(y_train2, y_pred_train2), 3))\n",
        "print(\"Test F1 score =\", round(f1_score(y_test2, y_pred_test2), 3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy = 93.60000000000001 %\n",
            "Test accuracy = 91.72 %\n",
            "\n",
            "Training F1 score = 0.936\n",
            "Test F1 score = 0.919\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}